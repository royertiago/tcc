\section{Medidas de Complexidade Computacional}
\label{sec:default_measures}

Nesta seção,
desceremos da ``estratosfera'',
em que lidávamos com os axiomas de Blum,
e trabalharemos com medidas de complexidade concretas.

\simbolo{$\mathcal B$}{Conjunto das funções booleanas}
\begin{provisionaldefinition}
    Seja $\mathcal B$ o conjunto das funções booleanas
    (isto é, cuja imagem é $\{0, 1\}$).
    Então,
    \begin{align*}
        \DTIME(f) &= \mathcal C_\PhiDT(f) \cap \mathcal B \\
        \DSPACE(f) &= \mathcal C_\PhiDS(f) \cap \mathcal B \\
        \NTIME(f) &= \mathcal C_\PhiNT(f) \cap \mathcal B \\
        \NSPACE(f) &= \mathcal C_\PhiNS(f) \cap \mathcal B
    \end{align*}
\end{provisionaldefinition}

(Isto é, $\DTIME(f)$ são todas as funções booleanas de $\mathcal C_\PhiDT(f)$.
Observe que existe uma correspondência direta entre
uma função booleana $g : \{0, 1\}^* \to \{0, 1\}$
e a linguagem correspondente $L = \{x \mid g(x) = 1\}$;
portanto,
usaremos os termos intercambiavelmente.)

Estas são as principais medias de complexidade computacional
para máquinas de Turing.

Esta noção de complexidade de espaço
é um pouco diferente da definição dada por
\citeonline[p.~285]{HopcroftUllman1979};
a definição deles exige apenas que,
ao computar $x$,
a máquina nunca leia mais do que $f(n)$ células da fita.
Note que não há exigência de parada;
desta forma,
a complexidade de espaço acaba não sendo uma medida de complexidade,
pois $\PhiDS(M, x)$ pode estar definido
mesmo quando $M(x)$ não está.
De fato,
são justamente estes casos que mereceram
atenção especial ao definir $\PhiDS$
da forma como definimos
e provar que esta função satisfaz ao axioma~\ref{ax:blum_def}.
Mesmo \citeauthoronline{HopcroftUllman1979}
notam que é necessário fazer este ajuste,
em \cite[p.~313]{HopcroftUllman1979}.

Outra diferença é o fato de nós permitirmos
às máquinas que computam linguagens em
(por exemplo) $\DSPACE(f)$
extrapolar o limite de $f(n)$ células
para um número finito de entradas.
Isto não costuma causar problemas;
podemos embutir no controle finito da máquina
a ``resposta certa''
para todas as palavras que violam o limite.

Entretanto,
mesmo assim,
a máquina lerá ao menos uma célula da fita de trabalho,
pois ela precisa realizar ao menos um movimento para escrever a resposta;
encontramos problemas caso $f(n) = 0$ seja zero em algum ponto.
Por exemplo, defina $f$ por
\begin{equation*}
    f(n) = \begin{cases}
        0, & n < 2 \\
        1, & n \geq 2
    \end{cases}.
\end{equation*}
Na definição de \citeonline{HopcroftUllman1979},
$\DTIME(f)$ é o conjunto vazio,
pois toda máquina de Turing
é obrigada a ler ao menos a célula inicial;%
\footnote{
    A única possível exceção
    é a máquina de Turing
    cujo estado inicial é igual ao estado final.
    Embora seja estranho falar de ``complexidade de espaço''
    de uma máquina que aceita a entrada sem sequer olhar um único símbolo,
    é admissível esta interpretação em que
    $\DSPACE(f) = \{\Sigma^*\}$.
}
enquanto que, em nossa definição,
$\DSPACE(f)$ corresponde ao conjunto das linguagens regulares.%
\footnote{
    Para provar isso, observe que,
    exceto um número finito de entradas,
    a máquina terá apenas uma célula de memória para usar.
    Então, ela pode armazenar esta informação no controle finito,
    funcionando como um autômato finito bidirecional
    (\emph{two"=way deterministic finite automaton}),
    que é equivalente a um autômato finito determinístico
    \cite[p.~40]{HopcroftUllman1979}.
}

Este exemplo é admitidamente forçado.
Qualquer máquina que queira aceitar alguma linguagem em $\DSPACE(f)$
é obrigada a violar a restrição de
ocupar menos espaço do que $f(n)$
para $n < 2$.
E, conforme discutimos dois parágrafos atrás,
é exatamente nestes exemplos forçados
em que as definições divergem.
Portanto, adotaremos esta hipótese de
\citeonline[p.~287]{HopcroftUllman1979}.

\begin{definition}
    \begin{align*}
        \DSPACE(f(n)) &= \mathcal C_\PhiDS(\max(f(n), 1)) \cap \mathcal B \\
        \NSPACE(f(n)) &= \mathcal C_\PhiNS(\max(f(n), 1)) \cap \mathcal B
    \end{align*}
\end{definition}

Para complexidade de tempo,
existem hipóteses similares.
\citeonline[p.~287]{HopcroftUllman1979}
assumem que $f(n) \geq n+1$
para complexidade de tempo.
A justificativa é que
qualquer máquina de Turing precisa de,
ao menos,
$n+1$ movimentos para ler o primeiro espaço em branco
após uma palavra de tamanho $n$.
Isto é,
este é o tempo mínimo necessário
para ler toda a entrada.
\citeonline[p.~33]{Papadimitriou1994}
adota uma suposição similar:
a de que $f(n) \geq n$.

Adotaremos a hipótese de \citeauthoronline{HopcroftUllman1979}.
\begin{definition}
    \begin{align*}
        \DTIME(f(n)) &= \mathcal C_\PhiDT(\max(f(n), n+1)) \cap \mathcal B \\
        \NTIME(f(n)) &= \mathcal C_\PhiNT(\max(f(n), n+1)) \cap \mathcal B
    \end{align*}
\end{definition}

Esta suposição é,
entretanto,
passível de objeções.
Existem máquinas de Turing
que aceitam uma entrada
sem ter de lê"=la por completo.
Podemos cumprir exigências como
$\PhiDT(M, x) \leq 2 + |x|/2$;
é uma situação um pouco diferente
daquela que tínhamos com complexidade de espaço,
em que éramos \emph{obrigados}
a violar as restrições de espaço
em alguns casos.

Uma ressalva:
é possível provar que
para qualquer linguagem em $\DTIME(n)$
(pela definição anterior),
existe algum número $k$ tal que
pertinência a $L(M)$
é determinada analisando"=se
apenas as primeiras $k$ letras da entrada.
Portanto as linguagens excluídas por esta hipótese
nem eram muito interessantes.

Existe mais uma inconsistência
em relação às classes de complexidade:
as constantes em frente às funções.
\citeonline[p.~25]{AroraBarak2009}
definem $\DTIME(f)$ como sendo o conjunto das linguagens
para as quais existem máquinas cujo tempo de execução
é menor que $c f(n)$, para alguma constante $c > 0$.
As definições de $\NTIME$, $\DSPACE$ e $\NSPACE$
de \citeonline[p.~41, p.~78, p.~79]{AroraBarak2009}
também permitem este fator constante.

Esta definição está de acordo com
a prática comum na análise de complexidade de algoritmos
de desprezar as constantes.
Ao menos,
neste caso,
conseguimos provar equivalência entre as definições.

\begin{theorem}
    Para toda constante $c > 0$,
    \begin{align*}
        \DSPACE(f) & = \DSPACE(cf) \\
        \NSPACE(f) & = \NSPACE(cf)
    \end{align*}
\end{theorem}

\begin{proof}
    Assuma sem perda de generalidade que $c < 1$.
    Seja $M$ uma máquina que $L(M) \in \DSPACE(f)$.
    O truque é representar várias células de $M$
    num único símbolo de fita.
    Mais precisamente,
    cada símbolo de $M'$ conterá
    $\lceil 1/c \rceil$ células de $M$.
    Como na complexidade de $M$
    não são contabilizados o tamanho da fita de entrada,
    a complexidade de $M'$ é menor do que $cf$.
\end{proof}

\begin{ucorollary}
    Nossas definições de complexidade de espaço
    são equivalentes à complexidade de espaço
    de \citeauthoronline{AroraBarak2009}.
\end{ucorollary}

Para complexidade de tempo,
a história não é tão bonita assim.
Precisamos separar em dois casos.

\begin{theorem}[Aceleramento linear\footnotemark]
    \footnotetext{Do inglês ``linear speedup''.}
    Se $f \in \omega(n)$, então
    \begin{align*}
        \DTIME(f) &= \DTIME(cf) \\
        \NTIME(f) &= \NTIME(cf)
    \end{align*}
\end{theorem}

Esta demonstração foi retirada de \cite[p.~290]{HopcroftUllman1979}.

\begin{proof}
    Assuma sem perda de generalidade que $c < 1$.
    Dada $M$ que aceita $L \in \DTIME(f)$,
    construiremos uma $M'$,
    necessariamente multifitas,
    que faz vários movimentos de $M$ de uma só vez.

    Fixe um valor de $r$ agora.
    A ideia é codificar trechos da fita de $M$
    com $r$ células
    na fita de $M'$,
    incluindo a posição da cabeça de leitura
    (se estiver lá),
    de maneira similar ao que fizemos com a complexidade de espaço.

    Para cada movimento,
    $M'$ irá ``carregar na memória cache''
    as células que estão sob o cabeçote de leitura
    e as células imediatamente à esquerda e à direita.
    Isto é, $M'$ armazenará esta informação
    no controle finito.
    Esta etapa custa quatro movimentos.

    Com $3r$ posições de memória de cada fita
    e o cabeçote de leitura nas $r$ posições centrais,
    $M'$ pode calcular todos os movimentos que $M$ faria nesta situação.
    Observe que,
    como estes movimentos dependem apenas
    das células da fita de $M$
    que agora estão no controle finito de $M'$,
    tal cálculo pode ser embutido nas regras de transição de estados de $M'$.
    Portanto, esta etapa é gratuita.
    Como a cabeça de leitura de $M$ estava nas $r$ posições centrais,
    acabamos de executar,
    no mínimo,
    $r$ movimentos de $M'$,
    sem custo de tempo.

    Agora, com mais quatro movimentos,
    nós ``submetemos'' as alterações da ``memória cache''
    na fita de $M'$.
    Ao final, com $8$ movimentos de $M'$,
    executamos ao menos $r$ movimentos de $M$.
    Portanto, após compactarmos a entrada
    neste formato,
    alcançaremos um estado de aceitação ou rejeição
    em, no máximo,
    \begin{equation*}
        \left\lceil \frac{8f(n)}{r} \right\rceil
    \end{equation*}
    etapas.

    O problema é,
    justamente,
    fazer esta compactação inicial.
    Podemos ler a entrada sequencialmente
    e ir apagando"=a,
    enquanto que a compactamos em outra fita
    (custo: $n$).
    Ao final,
    reposicione o cabeçote no começo
    (custo: $n/r$)
    e consideramos a fita de entrada como uma fita de trabalho
    e a fita com a entrada codificada
    como a fita de entrada.
    Custo:
    \begin{equation*}
        n + \left\lceil \frac n r \right\rceil.
    \end{equation*}
    Observe que assumimos
    que existem ao menos duas fitas disponíveis.

    Custo total:
    \begin{equation*}
        n + \left\lceil \frac n r \right\rceil +
            \left\lceil \frac{8f(n)}{r} \right\rceil
    \end{equation*}

    Como $f \in \omega(n)$, temos
    \begin{equation*}
        \lim_{n \rightarrow \infty} \frac{n}{f(n)} = 0.
    \end{equation*}
    Portanto, para $r > 8c$,
    podemos fazer o custo final
    ser menor que $cf(n)$ para todo $n$ suficientemente grande.
    Isso prova o teorema.
\end{proof}

\begin{theorem}
    \begin{align*}
        \DTIME(cn) &= \DTIME((1+\varepsilon)n) \\
        \NTIME(cn) &= \NTIME((1+\varepsilon)n)
    \end{align*}
    para qualquer $c > 1$ e $\varepsilon > 0$.
\end{theorem}

\begin{proof}
    Escolha $r = \varepsilon/16$ na demonstração do teorema anterior.
\end{proof}

\begin{ucorollary}
    Se existe algum $c > 1$ tal que
    \begin{equation*}
        f(n) \geq cn
    \end{equation*}
    para quase todo $n$,
    nossas definições de complexidade de tempo
    em relação à função $f$
    são equivalentes às de \citeauthoronline{AroraBarak2009}.
\end{ucorollary}

\citeonline[p.~32]{Papadimitriou1994}
dá uma caracterização elegante do aceleramento linear
que cobre os dois casos:
\begin{utheorem}
    Se $L$ é aceita em tempo $f(n)$ por alguma máquina de Turing,
    então $L$ é aceita em tempo $cf(n) + n + 2$ por alguma máquina de Turing,
    para qualquer $c$.
\end{utheorem}

Já provamos que nossa definição
é equivalente à de \citeauthoronline{AroraBarak2009}
(exceto no caso extremo
em que $f(n)$ fica eventualmente menor que $cn$
para todo $c > 1$).
Usando estes dois teoremas,
podemos provar a afirmação dada no início do capítulo
de que nossas definições e as de \citeonline{HopcroftUllman1979}
são equivalentes.

\begin{theorem}
    Se $L \in \DSPACE(f)$ (ou $L \in \NSPACE(f)$),
    então $L$ é decidida por uma máquina de Turing
    determinística (ou, respectivamente, não"=determinística)
    que jamais ocupa mais de $\max(f(n), 1)$
    posições de fita.
\end{theorem}

Observe que, em nossa definição de $\DSPACE$ e $\NSPACE$,
permitimos à máquina violar a restrição de
máximo de $f(n)$ células,
num número finito de instâncias.
De fato,
autorizamos"=na a nem parar nestes casos.
São justamente esses casos que precisam ser tratados.

\begin{proof}
    Como é um número finito de instâncias,
    podemos gravar todas elas no controle finito de $M'$,
    junto do \emph{status} destas palavras
    quanto à pertinência a $L(M)$.%
    \footnote{
        Observe que,
        embora saibamos que tais cadeias existem,
        e que $M$ possui comportamento não ambíguo
        nestas cadeias,
        não podemos usar um computador
        nem para encontrar todas as instâncias
        nem para determinar o comportamento de $M$ nessas instâncias.

        De certa forma,
        este teorema não é ``computável''.
    }

    $M'$ lerá a entrada
    e testar se é alguma das cadeias armazenadas.
    Caso a entrada pertença à lista embutida,
    aceite ou rejeite de acordo com o comportamento de $M$.
    Senão, retorne à posição inicial
    e execute $M$ na entrada.
    Isto pode ser feito usando técnicas
    para autômatos finitos,
    resultando num gasto nulo de espaço.

    Em qualquer caso,
    jamais ocuparemos mais de $\max(f(n), 1)$ células.

    Note que $M'$ é determinística
    se e só se $M$ o for.
\end{proof}

Complexidade de tempo é um pouco mais delicado.%
\footnote{
    Caso assumíssemos que $f(n) > cn$
    para algum $c > 1$,
    poderíamos agir como no teorema anterior
    e usar o aceleramento linear;
    mas queremos abranger casos
    como, por exemplo,
    $f(n) = n + 17$.
}

\begin{theorem}
    Se $L \in \DTIME(f)$ (ou $L \in \NTIME(f)$),
    então $L$ é decidida por uma máquina de Turing
    determinística (ou, respectivamente, não"=determinística)
    que jamais executa mais de $\max(f(n), n+1)$ movimentos.
\end{theorem}

\begin{proof}
    Suponha que $k$ seja o tamanho da maior palavra
    que viola à restrição de tempo.
    Manteremos, no controle finito,
    as $k$ primeiras células da fita de $M$.

    $M'$ começará deslocando"=se $k$ células à direita,
    populando este pedaço interno da fita.
    Mas serão feitas duas cópias deste pedaço interno.
    A primeira será populada apenas uma vez,
    e servirá para checar se a palavra
    pertence à linguagem,
    caso atinjamos o primeiro branco
    antes de realizar os $k$ primeiros movimentos.
    Neste caso, gastamos $|x|$ movimentos
    ($|x| - 1$ movimentos para deslocar"=se entre cada letra
    e mais $1$ para alcançar o caractere em branco),
    e com mais um movimento
    decidimos se aceitamos ou não a palavra,
    em tempo $|x| + 1$.

    A segunda cópia será tratada por nós
    como uma extensão da fita original.
    Ao executar o $k$"=ésimo movimento,
    $M'$ marcará a entrada com um caractere novo,
    e jamais mover"=se à esquerda deste caractere.
    Sempre que $M$ precisar ir àquela região,
    $M'$ simulará a atuação de $M$
    dentro do próprio controle finito,
    na segunda cópia das $k$ primeiras posições da fita.
    Esta alteração será feita apenas na fita de entrada;
    as demais fitas continuam operando normalmente.

    De fato, simularemos $M$ no controle finito de $M'$
    desde o início da execução do programa.
    Quando $M'$ efetuar os $k$ primeiros movimentos,
    $M$ já terá executado, dentro de si,
    $k$ movimentos diferentes.
    E, nas demais fitas, as posições das cabeças de leitura
    são exatamente as mesmas de $M$.
    Portanto,
    o funcionamento de $M'$
    é parecido com o de $M$
    --- exceto nos primeiros $k$ caracteres da entrada.

    Como cada movimento de $M$ corresponde a um movimento de $M'$,
    o limite de $f(n)$ de tempo é respeitado
    quando $n > k$.
    Juntando com a análise anterior
    (do que acontece quando a entrada é mais curta que $k$),
    concluímos que $M'$ aceita $L$ e respeita às exigências de tempo
    para todo $x$.
\end{proof}

Estes dois teoremas mostram que
toda linguagem em $\DSPACE(f)$,
de acordo com nossa definição,
também está em $\DSPACE(f)$
de acordo com as definições de \citeonline{HopcroftUllman1979}
e \citeonline{Papadimitriou1994}
(e afirmações análogas para $\DTIME$, $\NSPACE$ e $\NTIME$).

Finalizamos esta seção demostrando que
a premissa $f(n) \geq n+1$ para complexidade de tempo
apenas descarta linguagens que não são muito interessantes.

\begin{proposition}
    Suponha que $\PhiDT(M, x) \leq f(|x|)$ para alguma $f$,
    e $f(n_0) \leq n_0$ para algum $n_0$,
    então pertinência a $L(M)$
    é decidida analisando apenas os $n_0$ primeiros caracteres da palavra.
    \label{thm:sublinear_time_regular}
\end{proposition}

Observe que a hipótese é de que
$\PhiDT(M, x) \leq f(|x|)$,
não
$L(M) \in \DTIME(f)$.

\begin{proof}
    Para qualquer palavra $x$ de tamanho $n_0$,
    $M$ analisará,
    no máximo,
    até seu último caractere.
    Para chegar lá, $M$ gastará,
    no mínimo,
    $|x| - 1 = n_0 - 1$ movimentos;
    como
    \begin{equation*}
        \PhiDT(M, x) \leq f(|x|) = f(n_0) \leq n_0,
    \end{equation*}
    se $M$ ainda não está num estado final,
    o próximo movimento terá que ser para um
    --- caso contrário,
    violaria a equação acima.

    Observe que $M$ sequer pode analisar o caractere branco
    que vem após o $x$,
    pois precisaria de ao menos mais um movimento
    para atingir um estado final.
    Exatamente por isso, não importa o caractere que vem após o $x$.
    Qualquer palavra de tamanho maior que $n_0$
    pode ser quebrada em $xy$, com $|x| = n_0$.
    Como $M$ não pode violar a restrição discutida no parágrafo anterior,
    $M$ deve determinar pertinência de $xy$ à linguagem
    baseando"=se apenas em $x$.

    Como em nossa argumentação
    exigimos apenas que $|x| = n_0$,
    isso prova o teorema.
\end{proof}

Defina o conjunto $A$ como sendo as palavras de tamanho $n_0$ de $L(M)$,
e $B$ as palavras de tamanho menor que $n_0$.
Podemos, então, reformular a conclusão do teorema como
\begin{equation*}
    L(M) = A\Sigma^* \cup B,
\end{equation*}
em que $A$ e $B$ são conjuntos \emph{finitos}.

O converso também é verdadeiro
(qualquer linguagem dessa forma
satisfaz às conclusões do teorema anterior).
Portanto, são um subconjunto próprio
das linguagens regulares.

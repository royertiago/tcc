\section{Medidas de Complexidade Computacional}
\label{medidas_padrao}

\begin{definition}
    \begin{align*}
        \DTIME(f) &= \mathcal C_\PhiDT(f) \\
        \DSPACE(f) &= \mathcal C_\PhiDS(f) \\
        \NTIME(f) &= \mathcal C_\PhiNT(f) \\
        \NSPACE(f) &= \mathcal C_\PhiNS(f)
    \end{align*}
    Costumamos usar $T(n)$ para funções de classes de tempo
    e $S(n)$ para funções de classes de espaço.
\end{definition}

Estas são as principais medias de complexidade complutacional
para máquinas de Turing.

Importante notar que esta noção de complexidade de espaço
é um pouco diferente da definição dada por
\citeonline[p. 285]{HopcroftUllman1979};
a definição deles exige apenas que,
ao computar $x$,
a máquina nunca leia mais do que $f(n)$ células da fita.
Note que não há exigência de parada;
desta forma,
a complexidade de espaço acaba não sendo uma medida de complexidade,
pois $\PhiDS(M, x)$ pode estar definido
mesmo quando $M(x)$ não está.
De fato,
são justamente estes casos que mereceram
atenção especial ao definir $\PhiDS$
da forma como definimos
e provar que esta função satisfaz as axioma \ref{blum_def}.
Mesmo \citeauthoronline{HopcroftUllman1979}
notam que é necessário fazer este ajuste,
em \cite[p. 313]{HopcroftUllman1979}.

Outro problema é o fato de nós permitirmos
às máquinas computando linguagens em $\DSPACE(f)$,
por exemplo,
extrapolem o limite de $f$ células
para um número finito de entradas.
Tal relação não costuma causar problemas;
de fato,
podemos mostrar que,
caso $f(n) \geq 1$,
podemos retirar esta permissão
(a ideia é embutir,
no controle finito,
a lista das palavras que violam o limite de $f$ células).

Entretanto, é justamente nesta hipótese sobre $f$
que reside nosso problema. Defina
\begin{equation*}
    f(n) = \begin{cases}
        0, & n < 2 \\
        1, & n \geq 2
    \end{cases}.
\end{equation*}
Na definição de \citeonline{HopcroftUllman1979},
$\DSPACE(f)$ é o conjunto vazio,
pois toda máquina de Turing
é obrigada a ler ao menos a célula inicial\footnotemark;
enquanto que, em nossa definição,
$\DSPACE(f)$ corresponde ao conjunto das linguagens regulares.
\footnotetext{
    A única possível exceção
    é a máquina de Turing
    cujo estado inicial é igual ao estado final.
    Embora seja estranho falar de ``complexidade de espaço''
    de uma máquina que aceita a entrada sem sequer olhar um único símbolo,
    é admissível existir uma interpretação em que
    $\DSPACE(f) = \{\Sigma^*\}$.
}

Este exemplo é admitidamente forçado.
Qualquer máquina que queira aceitar alguma linguagem em $\DSPACE(f)$
é obrigada a violar a restrição de
ocupar menos espaço do que $f(n)$
para $n < 2$.
E, conforme discutimos dois parágrafos atrás,
é exatamente nestes exemplos forçados
em que as definições divergem.
Portanto, iremos seguir as ideias de
\citeonline[p. 287]{HopcroftUllman1979}
e assumir que $f(n) \geq 1$ para complexidades de espaço.

Para complexidade de tempo,
existem hipóteses similares.
\citeonline[p. 287]{HopcroftUllman1979}
assumem que $f(n) \geq n+1$
para complexidade de tempo.
A justificativa é que
qualquer máquina de Turing precisa de,
ao menos,
$n+1$ movimentos para ler o primeiro espaço em branco
após uma palavra de tamanho $n$.
Isto é,
este é o tempo mínimo necessário
para ler toda a entrada.
\citeonline[p. 33]{Papadimitriou1994}
adota uma suposição similar:
a de que $f(n) \geq n$.

Iremos adotar a hipótese de \citeauthoronline{HopcroftUllman1979}.

Esta suposição é,
entretanto,
sujeita a uma objeção:
existem máquinas de Turing
que aceitam uma entrada
sem ter de lê-la por completo.
Podemos cumprir exigências como
$\PhiDT(M, x) \leq 2 + |x|/2$;
é uma situação um pouco diferente
daquela que tínhamos com complexidade de espaço,
em que éramos \emph{obrigados}
a violar as restrições de espaço
em alguns casos.

Uma ressalva:
é possível provar que
qualquer linguagem em $DTIME(n)$
é a concatenação de uma linguagem finita
com $\Sigma^*$,
portanto as linguagens excluídas por esta hipótese
nem eram muito interessantes.

Uma prática comum à análise de complexidade de algoritmos
é desprezar as constantes. Podemos formalizar este ``desprezo'':

\begin{theorem}
    Para toda constante $c > 0$,
    \begin{align*}
        \DSPACE(f) & = \DSPACE(cf) \\
        \NSPACE(f) & = \NSPACE(cf)
    \end{align*}
\end{theorem}

\begin{proof}
    Assuma sem perda de generalidade que $c < 1$.
    Seja $M$ uma máquina que $L(M) \in \DSPACE(f)$.
    O truque é representar várias células de $M$
    num único símbolo de fita.
    Mais precisamente,
    cada símbolo de $M'$ conterá
    $\lceil 1/c \rceil$ células de $M$.
    Como na complexidade de $M$
    não são contablilizados o tamanho da fita de entrada,
    a complexidade de $M'$ é menor do que $cf$.
\end{proof}

Para complexidade de tempo,
a história não é tão bonita assim.
Precisamos separar em dois casos.

\begin{theorem}[Aceleramento linear \protect\footnotemark]
    \footnotetext{Do inglês ``linear speedup''.}
    Se $f \in \omega(n)$, então
    \begin{align*}
        \DTIME(f) &= \DTIME(cf) \\
        \NTIME(f) &= \NTIME(cf)
    \end{align*}
\end{theorem}

Esta demonstração foi retirada de \cite[p. 290]{HopcroftUllman1979}.

\begin{proof}
    Assuma sem perda de generalidade que $c < 1$.
    Dada $M$ que aceita $L \in \DTIME(f)$,
    iremos construir uma $M'$,
    necessariamente multifitas,
    que faz vários movimentos de $M$ de uma só vez.

    Fixe um valor de $r$ agora.
    A ideia é codificar trechos da fita de $M$
    com $r$ células
    na fita de $M'$,
    incluindo a posição da cabeça de leitura
    (se estiver lá),
    de maneira similar ao que fizemos com a complexidade de espaço.

    Para cada movimento,
    $M'$ irá ``carregar na memória cache''
    as células que estão sob o cabeçote de leitura
    e as células imediatamente à esquerda e à direita.
    Isto é, $M'$ irá armazenar esta informação
    no controle finito.
    Esta etapa custa quatro movimentos.

    Com $3r$ posições de memória de cada fita
    e o cabeçote de leitura nas $r$ posições centrais,
    $M'$ pode calcular todos os movimentos que $M$ faria nesta situação.
    Observe que,
    como estes movimentos dependem apenas
    das células da fita de $M$
    que agora estão no controle finito de $M'$,
    tal cálculo pode ser embutido nas regras de transição de estados de $M'$.
    Portanto, esta etapa é gratuita.
    Como a cabeça de leitura de $M$ estava nas $r$ posições centrais,
    acabamos de executar,
    no mínimo,
    $r$ movimentos de $M'$,
    sem custo de tempo.

    Agora, com mais quatro movimentos,
    nós ``submetemos'' as alterações da ``memória cache''
    na fita de $M'$.
    Ao final, com $8$ movimentos de $M'$,
    executamos ao menos $r$ movimentos de $M$.
    Portanto, após compactarmos a entrada
    neste formato,
    alcançaremos um estado de aceitação ou rejeição
    em, no máximo,
    \begin{equation*}
        \left\lceil \frac{8f(n)}{r} \right\rceil
    \end{equation*}
    etapas.

    O problema é,
    justamente,
    fazer esta compactação inicial.
    Podemos ler a entrada sequencialmente
    e ir apagando"=a,
    enquanto que a compactamos em outra fita
    (custo: $n$).
    Ao final,
    reposicione o cabeçote no começo
    (custo: $n/r$)
    e consideramos a fita de entrada como uma fita de trabalho
    e a fita com a entrada codificada
    como a fita de entrada.
    Custo:
    \begin{equation*}
        n + \left\lceil \frac n r \right\rceil.
    \end{equation*}
    Observe que assumimos
    que existem ao menos duas fitas disponíveis.

    Custo total:
    \begin{equation*}
        n + \left\lceil \frac n r \right\rceil +
            \left\lceil \frac{8f(n)}{r} \right\rceil
    \end{equation*}

    Como $f \in \omega(n)$, temos
    \begin{equation*}
        \lim_{n \rightarrow \infty} \frac{n}{f(n)} = 0.
    \end{equation*}
    Portanto, para $r > 8c$,
    podemos fazer o custo final
    ser menor que $cf(n)$ para todo $n$ suficientemente grande.
    Isso prova o teorema.
\end{proof}

\begin{theorem}
    \begin{align*}
        \DTIME(cn) &= \DTIME((1+\varepsilon)n) \\
        \NTIME(cn) &= \NTIME((1+\varepsilon)n)
    \end{align*}
    para qualquer $c > 1$ e $\varepsilon > 0$.
\end{theorem}

\begin{proof}
    Escolha $r = \varepsilon/16$ na demonstração do teorema anterior.
\end{proof}

\citeonline[p. 32]{Papadimitriou1994}
dá uma caracterização elegante do aceleramento linear
que cobre os dois casos:
\begin{utheorem}
    Se $L$ é aceita em tempo $f(n)$ por alguma máquina de Turing,
    então $L$ é aceita em tempo $cf(n) + n + 2$ por alguma máquina de Turing,
    para qualquer $c$.
\end{utheorem}

Por serem medidas de complexidade específicas,
podemos impor relações mais fortes entre elas
do que as que são fornecidas pelo teorema \ref{relacao_medidas}.

\begin{proposition}
    \begin{align}
        \DTIME(f) &\subseteq \NTIME(f) \label{dtime_in_ntime} \\
        \DSPACE(f) &\subseteq \NSPACE(f) \label{dspace_in_nspace} \\
        \DTIME(f) &\subseteq \DSPACE(f) \label{dtime_in_dspace} \\
        \NTIME(f) &\subseteq \DSPACE(f) \label{ntime_in_dspace}
    \end{align}
\end{proposition}
\begin{proof}
    \ref{dtime_in_ntime} e \ref{dspace_in_nspace} são consequências diretas
    do fato de que toda máquina determinística é,
    em particular, não determinística.

    Para \ref{dtime_in_dspace},
    note que, em $f(n)$ movimentos,
    a máquina pode ler, no máximo,
    $f(n)$ diferentes células
    --- afinal, no máximo uma célula nova pode ser lida a cada movimento.

    Para \ref{ntime_in_dspace},
    usaremos uma máquina com múltiplas fitas.

    Se $M$ é uma máquina que reconhece $L \in \NTIME(f)$,
    existe um limite na quantidade de possíveis transições
    que $M$ pode fazer em cada estado;
    digamos, $t$ transições diferentes.
    Cada cadeia sobre $\{0, \dots, t-1\}$ representa uma possível
    sequência de transições,
    que pode levar à parada ou não.

    Em uma das fitas,
    a nova máquina $M'$
    irá enumerar todas as palavras de $\{0, \dots, t-1\}^*$.
    Para cada palavra enumerada,
    $M'$ simulará $M$ na entrada,
    escolhendo as transições de acordo com a palavra
    que foi enumerada na outra fita.
    No evento de alguma transição ser para um estado final,
    consideraremos tal palavra ``fechada''.

    Se a transição for para um estado de aceitação,
    aceitamos a entrada;
    e, se todas as palavras de um mesmo tamanho $k$
    forem fechadas sem aceitação,
    nenhuma palavra
    codifica uma sequência de transições
    que leva a um estado de aceitação
    --- todas as menores que $k$ já foram analisadas
    e todas as maiores que $k$
    possuem uma palavra de tamanho $k$ que já foi fechada,
    fazendo com que a palavra inteira fique fechada.
    Nesta situação, podemos rejeitar a entrada.

    Como $L \in \NTIME(f)$,
    qualquer sequência de transições leva a algum estado final
    em, no máximo, $f(n)$ transições.
    Portanto,
    sabemos que iremos fechar todas as palavras
    de tamanho $f(n)$,

    Como as fitas de trabalho de $M$
    não ocupam mais do que $f(n)$ células,
    e a fita de enumeração de $M'$
    nunca precisará enumerar uma palavra mais longa que $f(n)$,
    a complexidade de espaço de $M'$ é $f(n)$.
    Concluímos que $L \in \DSPACE(f)$.
\end{proof}

\begin{theorem}
    Suponha que $f(n) \geq \log n$ para todo $n$.
    Se $L \in \DSPACE(f)$,
    então existe uma constante $c$
    tal que $L \in \DTIME(c^f)$.
\end{theorem}

\begin{proof}
    Seja $M$ uma máquina que aceita $L$ em espaço $f$.
    Conforme observado na equação \ref{num_possiveis_configuracoes},
    existem constantes $a$ e $c$ tais que,
    caso $M$ ocupe exatamente $k$ células na fita,
    existirão, no máximo,
    \begin{equation*}
        ac^k
    \end{equation*}
    diferentes configurações na fita.
    Para $k = f(n)$, a equação lê
    \begin{equation*}
        ac^{f(n)}.
    \end{equation*}
    Como $M$ é um decisor,
    $M$ encerra sua computação nesta quantidade de passos.

    $c^{f(n)} \geq n$, pois $f(n) \geq \log n$.
    Portanto, podemos usar o aceleramento linear
    para nos livrar daquela constante $a$,
    provando, assim, o teorema.
\end{proof}

\begin{theorem}
    Suponha que $f(n) \geq \log n$ para todo $n$.
    Se $L \in \NTIME(f)$,
    então existe uma constante $c$
    tal que $L \in \DTIME(c^f)$.
\end{theorem}

\begin{proof}
    Pela equação \ref{ntime_in_dspace},
    $L \in \DSPACE(f)$.
    Combinando com o teorema anterior,
    temos $L \in \DTIME(c^f)$ para algum $c$.
\end{proof}

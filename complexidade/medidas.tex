\section{Medidas de Complexidade Computacional}
\label{medidas_padrao}

\begin{pdefinition}
    \begin{align*}
        \DTIME(f) &= \mathcal C_\PhiDT(f) \\
        \DSPACE(f) &= \mathcal C_\PhiDS(f) \\
        \NTIME(f) &= \mathcal C_\PhiNT(f) \\
        \NSPACE(f) &= \mathcal C_\PhiNS(f)
    \end{align*}
\end{pdefinition}

Estas são as principais medias de complexidade complutacional
para máquinas de Turing.

Importante notar que esta noção de complexidade de espaço
é um pouco diferente da definição dada por
\citeonline[p. 285]{HopcroftUllman1979};
a definição deles exige apenas que,
ao computar $x$,
a máquina nunca leia mais do que $f(n)$ células da fita.
Note que não há exigência de parada;
desta forma,
a complexidade de espaço acaba não sendo uma medida de complexidade,
pois $\PhiDS(M, x)$ pode estar definido
mesmo quando $M(x)$ não está.
De fato,
são justamente estes casos que mereceram
atenção especial ao definir $\PhiDS$
da forma como definimos
e provar que esta função satisfaz as axioma \ref{blum_def}.
Mesmo \citeauthoronline{HopcroftUllman1979}
notam que é necessário fazer este ajuste,
em \cite[p. 313]{HopcroftUllman1979}.

Outro problema é o fato de nós permitirmos
às máquinas computando linguagens em $\DSPACE(f)$,
por exemplo,
extrapolem o limite de $f$ células
para um número finito de entradas.
Tal relação não costuma causar problemas;
de fato,
podemos mostrar que,
caso $f(n) \geq 1$,
podemos retirar esta permissão
(a ideia é embutir,
no controle finito,
a lista das palavras que violam o limite de $f$ células).

Entretanto, é justamente nesta hipótese sobre $f$
que reside nosso problema. Defina
\begin{equation*}
    f(n) = \begin{cases}
        0, & n < 2 \\
        1, & n \geq 2
    \end{cases}.
\end{equation*}
Na definição de \citeonline{HopcroftUllman1979},
$\DSPACE(f)$ é o conjunto vazio,
pois toda máquina de Turing
é obrigada a ler ao menos a célula inicial\footnotemark;
enquanto que, em nossa definição,
$\DSPACE(f)$ corresponde ao conjunto das linguagens regulares.
\footnotetext{
    A única possível exceção
    é a máquina de Turing
    cujo estado inicial é igual ao estado final.
    Embora seja estranho falar de ``complexidade de espaço''
    de uma máquina que aceita a entrada sem sequer olhar um único símbolo,
    é admissível existir uma interpretação em que
    $\DSPACE(f) = \{\Sigma^*\}$.
}

Este exemplo é admitidamente forçado.
Qualquer máquina que queira aceitar alguma linguagem em $\DSPACE(f)$
é obrigada a violar a restrição de
ocupar menos espaço do que $f(n)$
para $n < 2$.
E, conforme discutimos dois parágrafos atrás,
é exatamente nestes exemplos forçados
em que as definições divergem.
Portanto, iremos adotar esta hipótese de
\citeonline[p. 287]{HopcroftUllman1979}.

\begin{definition}
    \begin{align*}
        \DSPACE(f(n)) &= \mathcal C_\PhiDS(\max(f(n), 1)) \\
        \NSPACE(f(n)) &= \mathcal C_\PhiNS(\max(f(n), 1))
    \end{align*}
\end{definition}

Para complexidade de tempo,
existem hipóteses similares.
\citeonline[p. 287]{HopcroftUllman1979}
assumem que $f(n) \geq n+1$
para complexidade de tempo.
A justificativa é que
qualquer máquina de Turing precisa de,
ao menos,
$n+1$ movimentos para ler o primeiro espaço em branco
após uma palavra de tamanho $n$.
Isto é,
este é o tempo mínimo necessário
para ler toda a entrada.
\citeonline[p. 33]{Papadimitriou1994}
adota uma suposição similar:
a de que $f(n) \geq n$.

Iremos adotar a hipótese de \citeauthoronline{HopcroftUllman1979}.
\begin{definition}
    \begin{align*}
        \DTIME(f(n)) &= \mathcal C_\PhiDT(\max(f(n), n+1)) \\
        \NTIME(f(n)) &= \mathcal C_\PhiNT(\max(f(n), n+1))
    \end{align*}
\end{definition}

Esta suposição é,
entretanto,
passível de objeções.
Existem máquinas de Turing
que aceitam uma entrada
sem ter de lê-la por completo.
Podemos cumprir exigências como
$\PhiDT(M, x) \leq 2 + |x|/2$;
é uma situação um pouco diferente
daquela que tínhamos com complexidade de espaço,
em que éramos \emph{obrigados}
a violar as restrições de espaço
em alguns casos.

Uma ressalva:
é possível provar que
qualquer linguagem em $\DTIME(n)$
(pela definição anterior)
é a concatenação de uma linguagem finita
com $\Sigma^*$,
unida com outra linguagem finita;
portanto as linguagens excluídas por esta hipótese
nem eram muito interessantes.

Existe mais uma inconsistência
em relação às classes de complexidade:
as constantes em frente às funções.
\citeonline[p. 25]{AroraBarak2009}
definem $\DTIME(f)$ como sendo o conjunto das linguagens
para as quais existem máquinas cujo tempo de execução
é menor que $c f(n)$, para alguma constante $c > 0$.
As definições de $\NTIME$, $\DSPACE$ e $\NSPACE$
de \citeonline[p. 41, p. 78, p. 79]{AroraBarak2009}
também permitem este fator constante.

Esta definição está de acordo com
a prática comum na análise de complexidade de algoritmos
de desprezar as constantes.
Ao menos,
neste caso,
conseguimos provar equivalência entre as definições.

\begin{theorem}
    Para toda constante $c > 0$,
    \begin{align*}
        \DSPACE(f) & = \DSPACE(cf) \\
        \NSPACE(f) & = \NSPACE(cf)
    \end{align*}
\end{theorem}

\begin{proof}
    Assuma sem perda de generalidade que $c < 1$.
    Seja $M$ uma máquina que $L(M) \in \DSPACE(f)$.
    O truque é representar várias células de $M$
    num único símbolo de fita.
    Mais precisamente,
    cada símbolo de $M'$ conterá
    $\lceil 1/c \rceil$ células de $M$.
    Como na complexidade de $M$
    não são contablilizados o tamanho da fita de entrada,
    a complexidade de $M'$ é menor do que $cf$.
\end{proof}

\begin{ucorollary}
    Nossas definições de complexidade de espaço
    são equivalentes à complexidade de espaço
    de \citeauthoronline{AroraBarak2009}.
\end{ucorollary}

Para complexidade de tempo,
a história não é tão bonita assim.
Precisamos separar em dois casos.

\begin{theorem}[Aceleramento linear \protect\footnotemark]
    \footnotetext{Do inglês ``linear speedup''.}
    Se $f \in \omega(n)$, então
    \begin{align*}
        \DTIME(f) &= \DTIME(cf) \\
        \NTIME(f) &= \NTIME(cf)
    \end{align*}
\end{theorem}

Esta demonstração foi retirada de \cite[p. 290]{HopcroftUllman1979}.

\begin{proof}
    Assuma sem perda de generalidade que $c < 1$.
    Dada $M$ que aceita $L \in \DTIME(f)$,
    iremos construir uma $M'$,
    necessariamente multifitas,
    que faz vários movimentos de $M$ de uma só vez.

    Fixe um valor de $r$ agora.
    A ideia é codificar trechos da fita de $M$
    com $r$ células
    na fita de $M'$,
    incluindo a posição da cabeça de leitura
    (se estiver lá),
    de maneira similar ao que fizemos com a complexidade de espaço.

    Para cada movimento,
    $M'$ irá ``carregar na memória cache''
    as células que estão sob o cabeçote de leitura
    e as células imediatamente à esquerda e à direita.
    Isto é, $M'$ irá armazenar esta informação
    no controle finito.
    Esta etapa custa quatro movimentos.

    Com $3r$ posições de memória de cada fita
    e o cabeçote de leitura nas $r$ posições centrais,
    $M'$ pode calcular todos os movimentos que $M$ faria nesta situação.
    Observe que,
    como estes movimentos dependem apenas
    das células da fita de $M$
    que agora estão no controle finito de $M'$,
    tal cálculo pode ser embutido nas regras de transição de estados de $M'$.
    Portanto, esta etapa é gratuita.
    Como a cabeça de leitura de $M$ estava nas $r$ posições centrais,
    acabamos de executar,
    no mínimo,
    $r$ movimentos de $M'$,
    sem custo de tempo.

    Agora, com mais quatro movimentos,
    nós ``submetemos'' as alterações da ``memória cache''
    na fita de $M'$.
    Ao final, com $8$ movimentos de $M'$,
    executamos ao menos $r$ movimentos de $M$.
    Portanto, após compactarmos a entrada
    neste formato,
    alcançaremos um estado de aceitação ou rejeição
    em, no máximo,
    \begin{equation*}
        \left\lceil \frac{8f(n)}{r} \right\rceil
    \end{equation*}
    etapas.

    O problema é,
    justamente,
    fazer esta compactação inicial.
    Podemos ler a entrada sequencialmente
    e ir apagando"=a,
    enquanto que a compactamos em outra fita
    (custo: $n$).
    Ao final,
    reposicione o cabeçote no começo
    (custo: $n/r$)
    e consideramos a fita de entrada como uma fita de trabalho
    e a fita com a entrada codificada
    como a fita de entrada.
    Custo:
    \begin{equation*}
        n + \left\lceil \frac n r \right\rceil.
    \end{equation*}
    Observe que assumimos
    que existem ao menos duas fitas disponíveis.

    Custo total:
    \begin{equation*}
        n + \left\lceil \frac n r \right\rceil +
            \left\lceil \frac{8f(n)}{r} \right\rceil
    \end{equation*}

    Como $f \in \omega(n)$, temos
    \begin{equation*}
        \lim_{n \rightarrow \infty} \frac{n}{f(n)} = 0.
    \end{equation*}
    Portanto, para $r > 8c$,
    podemos fazer o custo final
    ser menor que $cf(n)$ para todo $n$ suficientemente grande.
    Isso prova o teorema.
\end{proof}

\begin{theorem}
    \begin{align*}
        \DTIME(cn) &= \DTIME((1+\varepsilon)n) \\
        \NTIME(cn) &= \NTIME((1+\varepsilon)n)
    \end{align*}
    para qualquer $c > 1$ e $\varepsilon > 0$.
\end{theorem}

\begin{proof}
    Escolha $r = \varepsilon/16$ na demonstração do teorema anterior.
\end{proof}

\begin{ucorollary}
    Se existe algum $c > 1$ tal que
    \begin{equation*}
        f(n) \geq cn
    \end{equation*}
    para quase todo $n$,
    nossas definições de complexidade de tempo
    em relação à função $f$
    são equivalentes às de \citeauthoronline{AroraBarak2009}.
\end{ucorollary}

\citeonline[p. 32]{Papadimitriou1994}
dá uma caracterização elegante do aceleramento linear
que cobre os dois casos:
\begin{utheorem}
    Se $L$ é aceita em tempo $f(n)$ por alguma máquina de Turing,
    então $L$ é aceita em tempo $cf(n) + n + 2$ por alguma máquina de Turing,
    para qualquer $c$.
\end{utheorem}

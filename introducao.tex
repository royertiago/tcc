\chapter{INTRODUÇÃO}

A teoria da computabilidade
classifica os problemas computacionais entre
aqueles que podem e não podem ser resolvidos mecanicamente.
Existem diversos dispositivos matemáticos
que tentam capturar esta noção de
``resolvidos mecanicamente'';
o modelo que usaremos neste trabalho é a máquina de Turing.

Dizemos que um problema é \emph{decidível}
se ele puder ser resolvido por uma máquina de Turing.
Embora as máquinas de Turing
não se pareçam com os computadores modernos,
estes dois dispositivos são equivalentes,
no sentido de que
todos os problemas resolvíveis por um deles
também é resolvível pelo outro.
\footnote{
    Eles são equivalentes até certo ponto.
    Máquinas de Turing são objetos matemáticos
    com ``memória infinita'',
    enquanto que os computadores estão limitados
    a uma quantidade finita de memória;
    esta limitação impõe restrições no tamanho das entradas
    que não existem nas máquinas de Turing.
}
Portanto, podemos pensar nos problemas decidíveis
como aqueles que podem ser resolvidos com o uso de um computador
\cite[p.~307]{HopcroftMotwaniUllman2001}.
Embora a maior parte dos problemas
encontrados no dia"=a"=dia seja decidível,
existem alguns problemas de importância teórica e prática
que não o são.
% TODO: Adicionar citação aqui.
Um exemplo notável é determinar se dois programas são equivalentes.
\footnote{
    Este problema possui importância prática:
    caso existisse um algoritmo para determinar equivalência entre programas,
    poderíamos usar um computador para saber se
    determinado refatoramento ou otimização de código
    não alterou acidentalmente o significado do programa;
    isto é, a mudança não introduziu um bug.
}

Entretanto,
apenas ser decidível não é suficiente.
Existem alguns problemas
(de fato, categorias inteira deles)
que, por mais rápido que seja o computador
que esteja tentando resolvê"=lo,
o tempo necessário torna"=se excessivamente grande
muito rapidamente.
Chamamos estes problemas de \emph{intratáveis}
\cite[p.~1]{HopcroftMotwaniUllman2001}
Por mais que eles sejam decidíveis,
pode ser que demore até o Sol esfriar
(evento que estima"=se que vá ocorrer em 5 bilhões de anos)
para que o computador devolva a solução.

É aqui que entra a teoria de complexidade computacional.
Essencialmente,
\emph{complexidade}
é a quantidade de recursos que são gastos
para obter a solução de determinado problema.
Geralmente nos concentramos em um único recurso;
os dois principais são
o tempo de execução e a quantidade de memória usada.

A complexidade computacional classifica os problemas
de acordo com os recursos necessários para que sejam resolvidos.
Por exemplo,
a classe de complexidade $\P$
é o conjunto de todos os problemas que podem ser resolvidos
por uma máquina de Turing determinística
em tempo polinomial;
isto é,
alguma máquina de Turing determinística
resolve as instâncias de tamanho $n$
usando no máximo $p(n)$ unidades de tempo,
para algum polinômio $p$.
Similarmente,
a classe $\NP$
contempla os problemas resolvíveis em tempo polinomial
por máquinas de Turing \emph{não} determinísticas.

Estes dois conjuntos protagonizam
o maior e mais importante problema da teoria de complexidade computacional,
e um dos mais importantes problemas em aberto de toda a matemática.
Ei"=lo:
\begin{displaymath}
    \P \stackrel?= \NP
\end{displaymath}
\cite[p.~270]{Sipser2006}.
Sabemos que $\P \subseteq \NP$,
pois toda máquina determinística pode ser vista como
uma máquina não"=deterministica que escolheu não ``usar''
seu não"=determinismo.
A maior parte dos pesquisadores acredita que
esta inclusão é estrita \cite[p.~2]{Gasarch2012}.

Os modelos determinístico e não"=determinístico de máquinas de Turing são equivalentes,
no sentido de que qualquer problema que pode ser resolvido por um modelo
também pode ser resolvido por outro.
Entretanto, o modelo não"=determinístico
aparenta ser exponencialmente mais rápido que
o modelo determinístico;
de fato, para um subconjunto vasto de $\NP$,
os problemas $\NP$"=completos
(considerados os mais difíceis da classe $\NP$),
são apenas conhecidos apenas algoritmos determinísticos exponenciais.

Curiosamente,
o ganho exponencial de desempenho
só é atingido se considerarmos o tempo de execução.
A complexidade de espaço permite apenas um ganho quadrático;
este resultado é conhecido como teorema de Savitch.

Apesar desta discrepância entre o ganho de eficiência via não"=determinismo,
complexidade de tempo e complexidade de espaço
compartilham muitos teoremas semelhantes.
\citeonline{Blum1967} desenvolveu uma teoria de complexidade axiomática.
Ele formalizou a noção de ``recurso computacional''
através de dois axiomas,
aplicáveis a qualquer modelo de máquina que compute funções de inteiros.
\footnote{
    Embora usamos o termo ``funções de inteiros'',
    as funções tratadas possuem~$\mathbb N$ como domínio e contradomínio.
    (Eu mantive a terminologia ``funções de inteiros''
    para ser compatível com o termo em inglês \emph{integer functions},
    utilizado por \citeonline[p.~151]{HopcroftUllman1979}.)
}
Usando apenas estes dois axiomas,
muitos teoremas a respeito de complexidade computacional,
que costumam ser apresentados separadamente em livros didáticos sobre o assunto,
podem ser demonstrados de maneira universal.

Para máquinas de Turing determinísticas,
os resultados são facilmente aplicados
--- podemos, por exemplo,
entender o conteúdo inicial da fita como a entrada da função
e o conteúdo da fita quando a máquina para como a saída da função.

Entretanto, para máquinas de Turing não"=determinísticas,
há um obstáculo elementar:
os axiomas de Blum dizem respeito a funções.
O que é uma ``função não"=determinística''?

\citeonline[p.~313]{HopcroftUllman1979} dão uma possível definição:
uma máquina não"=determinística computa uma função se,
e somente se,
todos os ramos de computação encerram com a mesma palavra na fita;
neste caso,
esta palavra é a saída da função para a respectiva entrada.

Esta definição não é satisfatória;
isto é, ela não generaliza a noção de não"=determinismo para decisores.
Se todos os ramos de computação encerram com a mesma palavra,
poderiamos simplesmente fixar uma das possíveis escolhas na função de transição;
afinal, o resultado sempre será o mesmo.
Portanto, em essência,
temos o poder computacional de uma máquina determinística.

Isso ignora a noção de que máquinas não"=determinísticas
aparentemente possuem velocidade de execução exponencialmente maior
do que máquinas determinísticas,
impedindo que relacionemos axiomas de Blum
ao problema $\P$~vs~$\NP$.

Neste trabalho,
proporei uma nova definição de função não"=determinística,
de maneira a preservar este aparente ganho exponencial de tempo de execução,
permitindo, portanto,
um paralelo com o problema $\P$~vs~$\NP$.

Será assumido que o leitor possui conhecimento básico
sobre máquinas de Turing e a teoria da computabilidade.
As definições básicas deste dispositivo
encontram"=se no apêndice~\ref{maqs_turing}.

\section{Objetivos}

Estabelecer relações entre classes de complexidade computacional
e complexidade de circuitos.

\subsection{Objetivos Especificos}

\begin{enumerate}
    \item Estudar as classes de complexidade computacional
        e complexidade de circuitos.
    \item Demonstrar a relação entre complexidade de circuitos
        e o problema $\P$ versus $\NP$.
    \item Correlacionar as principais classes
        de complexidade computacional
        às correspondentes classes de complexidade de circuitos
    \item Estabelecer limites inferiores no tamanho
        e profundidade dos circuitos
        que computam certas funções recursivas.
\end{enumerate}

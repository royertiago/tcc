\chapter{INTRODUÇÃO}

A teoria da computabilidade
classifica os problemas computacionais entre
aqueles que podem e não podem ser resolvidos mecanicamente.
Existem diversos dispositivos matemáticos
que tentam capturar esta noção de
``resolvidos mecanicamente'';
o modelo que usaremos neste trabalho é a máquina de Turing.

Dizemos que um problema é \emph{decidível}
se ele puder ser resolvido por uma máquina de Turing.
Embora as máquinas de Turing
não se pareçam com os computadores modernos,
estes dois dispositivos são equivalentes,
no sentido de que
todos os problemas resolvíveis por um deles
também é resolvível pelo outro.%
\footnote{
    Eles são equivalentes até certo ponto.
    Máquinas de Turing são objetos matemáticos
    com ``memória infinita'',
    enquanto que os computadores estão limitados
    a uma quantidade finita de memória;
    esta limitação impõe restrições no tamanho das entradas
    que não existem nas máquinas de Turing.
}
Portanto, podemos pensar nos problemas decidíveis
como aqueles que podem ser resolvidos com o uso de um computador
\cite[p.~307]{HopcroftMotwaniUllman2001}.
Embora a maior parte dos problemas
encontrados no dia"=a"=dia seja decidível,
existem alguns problemas de importância teórica e prática
que não o são.
% TODO: Adicionar citação aqui.
Um exemplo notável é determinar se dois programas são equivalentes.%
\footnote{
    Este problema possui importância prática:
    caso existisse um algoritmo para determinar equivalência entre programas,
    poderíamos usar um computador para saber se
    determinado refatoramento ou otimização de código
    não alterou acidentalmente o significado do programa;
    isto é, a mudança não introduziu um bug.
}

Entretanto,
apenas ser decidível não é suficiente.
Existem alguns problemas
(de fato, categorias inteiras deles)
que, por mais rápido que seja o computador
que esteja tentando resolvê"=lo,
o tempo necessário torna"=se excessivamente grande
muito rapidamente.
Chamamos estes problemas de \emph{intratáveis}
\cite[p.~1]{HopcroftMotwaniUllman2001}
Por mais que eles sejam decidíveis,
pode ser que demore até o Sol esfriar
(evento que, estima"=se, vai ocorrer em 5 bilhões de anos)
para que o computador devolva a solução.

É aqui que entra a teoria de complexidade computacional.
Essencialmente,
\emph{complexidade}
é a quantidade de recursos que são gastos
para obter a solução de determinado problema.
Geralmente nos concentramos em um único recurso;
os dois principais são
o tempo de execução e a quantidade de memória usada.

A complexidade computacional classifica os problemas
de acordo com os recursos necessários para que sejam resolvidos.
Por exemplo,
a classe de complexidade $\P$
é o conjunto de todos os problemas que podem ser resolvidos
por uma máquina de Turing determinística
em tempo polinomial;
isto é,
alguma máquina de Turing determinística
resolve qualquer instância de tamanho $n$
usando no máximo $p(n)$ unidades de tempo,
para algum polinômio $p$.
Similarmente,
a classe $\NP$
contempla os problemas resolvíveis em tempo polinomial
por máquinas de Turing \emph{não} determinísticas.

Estes dois conjuntos protagonizam
o maior e mais importante problema da teoria de complexidade computacional,
e um dos mais importantes problemas em aberto de toda a matemática.
Ei"=lo:
\begin{equation*}
    \P \stackrel?= \NP
\end{equation*}
\cite[p.~270]{Sipser2006}.
Sabemos que $\P \subseteq \NP$,
pois toda máquina determinística pode ser vista como
uma máquina não"=determinística que escolheu não ``usar''
seu não"=determinismo.
A maior parte dos pesquisadores acredita que
esta inclusão é estrita \cite[p.~54]{Gasarch2012}.

Os modelos determinístico e não"=determinístico de máquinas de Turing são equivalentes,
no sentido de que qualquer problema que pode ser resolvido por um modelo
também pode ser resolvido por outro.
Entretanto, o modelo não"=determinístico
aparenta ser exponencialmente mais rápido que
o modelo determinístico;
de fato, para um subconjunto vasto de $\NP$,
os problemas $\NP$"=completos
(considerados os mais difíceis da classe $\NP$),
são conhecidos apenas algoritmos determinísticos exponenciais.

Curiosamente,
o ganho exponencial de desempenho
só é atingido se considerarmos o tempo de execução.
A complexidade de espaço permite apenas um ganho quadrático;
este resultado é conhecido como teorema de Savitch.

Apesar desta discrepância entre o ganho de eficiência via não"=determinismo,
complexidade de tempo e complexidade de espaço
compartilham muitos teoremas semelhantes;
isso sugere que eles são casos particulares de uma noção abstrata,
a de ``recurso computacional''.
\citeonline[p.~324]{Blum1967} formalizou a noção de ``medida de complexidade''
através de dois axiomas,
aplicáveis a qualquer modelo de máquina que compute funções de inteiros.%
\footnote{
    Embora usamos o termo ``funções de inteiros'',
    as funções tratadas possuem~$\mathbb N$ como domínio e contradomínio.
    (Eu mantive a terminologia ``funções de inteiros''
    para ser compatível com o termo em inglês \emph{integer functions},
    utilizado por \citeonline[p.~151]{HopcroftUllman1979}.)
}
Usando apenas estes dois axiomas,
muitos teoremas a respeito de complexidade computacional,
que costumam ser apresentados separadamente em livros didáticos sobre o assunto,
podem ser demonstrados de maneira universal.

Para máquinas de Turing determinísticas,
os resultados são facilmente aplicados;
isto é,
é intuitivo interpretar uma máquina de Turing determinística
como um computador de alguma função.
Podemos, por exemplo,
entender o conteúdo inicial da fita como a entrada da função
e o conteúdo da fita quando a máquina para como a saída da função.

Entretanto, para máquinas de Turing não"=determinísticas,
há um obstáculo elementar:
os axiomas de Blum dizem respeito a funções.
O que é uma ``função não"=determinística''?

\citeonline[p.~313]{HopcroftUllman1979} dão uma possível definição:
uma máquina não"=determinística computa uma função se,
e somente se,
todos os ramos de computação encerram com a mesma palavra na fita;
neste caso,
esta palavra é a saída da função para a respectiva entrada.

Esta definição não é satisfatória;
isto é, ela não generaliza a noção de não"=determinismo para decisores.
Se todos os ramos de computação encerram com a mesma palavra,
poderíamos simplesmente fixar uma das possíveis escolhas na função de transição;
afinal, o resultado sempre será o mesmo.
Portanto, em essência,
temos o poder computacional de uma máquina determinística.

Isso ignora a noção de que máquinas não"=determinísticas
aparentemente possuem velocidade de execução exponencialmente maior
do que máquinas determinísticas,
impedindo que relacionemos axiomas de Blum
ao problema $\P$~vs~$\NP$.

Neste trabalho,
proporei uma nova definição de função não"=determinística,
de maneira a preservar este aparente ganho exponencial de tempo de execução,
permitindo, portanto,
um paralelo com o problema $\P$~vs~$\NP$.

\section{OBJETIVOS}

Desenvolver uma nova definição de funções não"=determinísticas,
de modo a generalizar naturalmente o aparente aceleramento exponencial
que obtemos ao conceder não"=determinismo à máquinas de Turing.

\subsection[Objetivos específicos]{Objetivos Específicos\footnotemark}
\footnotetext{
    Como o objetivo geral é definir um novo conceito,
    os objetivos específicos são voltados a validá"=lo.
}

\begin{enumerate}
    \item Manter compatibilidade com os axiomas de Blum.
    \item Comparar a definição dada com o trabalho de outros autores.
    \item Desenvolver a noção de composição de funções não"=determinísticas.
    \item Relacionar a composição de funções não"=determinísticas
        com a hierarquia polinomial.
\end{enumerate}

\section{ESTRUTURAÇÃO DO TEXTO}

Os axiomas de Blum definem a noção de complexidade computacional
não apenas para máquinas de Turing,
mas sim para qualquer modelo de computação que satisfaça algumas restrições.
O capítulo~\ref{ch:enumeration_of_recursive_functions}
contém a maquinaria matemática para formular,
precisamente,
quais são estas restrições.
Este capítulo discorre sobre funções recursivas,
formas de enumerá"=las
e características desejáveis destas enumerações.

O capítulo~\ref{ch:computational_complexity}
apresenta os axiomas de Blum
e o conceito associado de classes de complexidade computacional.
Também são definidas formalmente
as duas principais classes de complexidade computacional
--- $\P$ e $\NP$.
Neste capítulo aparece o teorema da união,
que justifica
(do ponto de vista da definição axiomática de ``complexidade'')
chamar as classes $\P$, $\NP$, $\PSPACE$ etc.\ de ``classes de complexidade''.

A noção de ``oráculo computacional'' é explorada no capítulo~\ref{ch:oracles}.
Aqui,
são definidas as hierarquias aritmética e polinomial.

Finalmente,
no capítulo~\ref{ch:nondeterministic_functions},
é definida ``função não"=determinística''.
Podemos finalmente estender a noção de complexidade computacional
para máquinas de Turing não"=determinísticas.
Também é definido ``oráculo funcional'',
que permite construir uma hierarquia de funções
análoga às hierarquias aritmética e polinomial.
E, atingindo um dos objetivos do TCC,
demonstramos que o fecho compositivo da classe $\Sigma_n^f$
é a classe $\Delta_{n+1}^f$;
de certa forma,
encontramos uma relação na hierarquia polinomial
através da composição de funções não"=determinísticas.

Na maior parte do texto,
o conceito de ``máquina de Turing'' é tratado informalmente.
Entretanto,
algumas afirmações feitas no texto
(por exemplo, a de que uma representação em binário
identifica unicamente uma máquina de Turing)
depende da definição formal deste dispositivo
--- isto é, os ``detalhes de implementação''.
Estes detalhes estão no apêndice~\ref{app:turing_machines}.
